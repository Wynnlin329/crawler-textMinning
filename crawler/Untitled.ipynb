{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "# import pprint\n",
    "import pandas as pd\n",
    "import re\n",
    "import threading\n",
    "import random\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: './104/synonym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-4fb2949da610>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mword_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdictionary_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'%s/%s'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mword_list\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: './104/synonym'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load dictionary\n",
    "path = './104'\n",
    "dictionary_list = os.listdir(path)\n",
    "word_list = list()\n",
    "for f in dictionary_list:\n",
    "    with open(r'%s/%s'%(path, f), 'r') as d:\n",
    "        word_list += d.read().split('\\n')\n",
    "\n",
    "# Header\n",
    "headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "   'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "   'Accept-Encoding': 'none',\n",
    "   'Accept-Language': 'en-US,en;q=0.8',\n",
    "   'Connection': 'keep-alive'}\n",
    "\n",
    "# Create a directory to saving files\n",
    "path = r'./job104_resource'\n",
    "if not os.path.isdir(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "work_path = r'./work_dir'\n",
    "if not os.path.isdir(work_path):\n",
    "    os.mkdir(work_path)\n",
    "\n",
    "# synonym dictionary\n",
    "synonym_dict = {}\n",
    "synonym_path = r'./synonym/synonym.txt'\n",
    "with open(synonym_path, 'r') as syn:\n",
    "    syn_str = syn.read().split('\\n')\n",
    "for each_row in syn_str:\n",
    "    synonym_dict[each_row.split(',')[0]] = [item for item in each_row.split(',')]\n",
    "\n",
    "# Jieba then replace by synonym dictionary\n",
    "def dealWithSynonym(long_str):\n",
    "    # Select words we need according to /dict\n",
    "    # and append each word to tmp_list\n",
    "    tmp_list = []\n",
    "    long_str.replace(' ','')\n",
    "    for word_select in word_list:\n",
    "        if word_select in long_str:\n",
    "            if word_select.upper() == 'JAVA' or word_select.upper() == 'JAVASCRIPT':\n",
    "                continue\n",
    "            elif word_select == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0] != 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) != None:\n",
    "                # print(1,word_select)\n",
    "                long_str = long_str.replace(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0], '')\n",
    "                # print(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0])\n",
    "                if word_select in long_str:\n",
    "                    tmp_list.append(word_select.upper())\n",
    "                # print(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0])\n",
    "                continue\n",
    "            elif word_select == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) == None:\n",
    "                # print(2,word_select)\n",
    "                continue\n",
    "            else:\n",
    "                # print(3,word_select)\n",
    "                tmp_list.append(word_select.upper())\n",
    "    long_str = long_str.upper()\n",
    "    for word_select in word_list:\n",
    "        if (word_select.upper() in long_str) and (not word_select.upper() in tmp_list):\n",
    "            if word_select.upper() == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0] != 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) != None:\n",
    "                # print(1, word_select)\n",
    "                long_str = long_str.replace(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0], '')\n",
    "                if re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) == 'R':\n",
    "                    tmp_list.append(word_select.upper())\n",
    "                # print(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0])\n",
    "                continue\n",
    "            elif word_select.upper() == 'JAVA':\n",
    "                continue\n",
    "            elif word_select.upper() == 'JAVASCRIPT':\n",
    "                long_str = long_str.replace('JAVASCRIPT', '')\n",
    "                tmp_list.append(word_select.upper())\n",
    "            elif word_select.upper() == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) == None:\n",
    "                # print(2, word_select)\n",
    "                continue\n",
    "            else:\n",
    "                # print(3, word_select)\n",
    "                tmp_list.append(word_select.upper())\n",
    "    long_str = long_str.replace('JAVASCRIPT', '')\n",
    "    for word_select in word_list:\n",
    "        if (word_select.upper() in long_str) and (not word_select.upper() in tmp_list):\n",
    "            if word_select.upper() == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0] != 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) != None:\n",
    "                # print(1, word_select)\n",
    "                long_str = long_str.replace(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0], '')\n",
    "                if re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) == 'R':\n",
    "                    tmp_list.append(word_select.upper())\n",
    "                # print(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0])\n",
    "                continue\n",
    "            elif word_select.upper() == 'JAVA':\n",
    "                tmp_list.append(word_select.upper())\n",
    "            elif word_select.upper() == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) == None:\n",
    "                # print(2, word_select)\n",
    "                continue\n",
    "            else:\n",
    "                # print(3, word_select)\n",
    "                tmp_list.append(word_select.upper())\n",
    "\n",
    "    # Replace Synonym\n",
    "    for word_key in synonym_dict:\n",
    "        for word_value in  synonym_dict[word_key]:\n",
    "            for num, operating_word in enumerate(tmp_list):\n",
    "                if operating_word.upper() == word_value.upper():\n",
    "                    tmp_list[num] = word_key\n",
    "    tmp_list = list(set(tmp_list))\n",
    "\n",
    "    tmp_str = ''\n",
    "    for n, w in enumerate(tmp_list):\n",
    "        tmp_str += w\n",
    "        if n < len(tmp_list) - 1:\n",
    "            tmp_str += ','\n",
    "\n",
    "    return tmp_str\n",
    "\n",
    "def getSkill(titleUrl):\n",
    "    job_content = '工作內容'\n",
    "    job_require = '條件要求'\n",
    "    job_welfare = '公司福利'\n",
    "    job_contact = '聯絡方式'\n",
    "    tmp_list = [job_content, job_require, job_welfare, job_contact]\n",
    "\n",
    "    job_skill_data = ''\n",
    "\n",
    "    try:\n",
    "        res = requests.get(titleUrl, headers=headers)\n",
    "    except:\n",
    "        print('Error', 'getSkill(titleUrl)', titleUrl)\n",
    "        print('Wait a moment!')\n",
    "        time.sleep(4)\n",
    "        return tmp_list[0], tmp_list[1], tmp_list[2], tmp_list[3], job_skill_data\n",
    "\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    contents = soup.select('section[class=\"info\"]')\n",
    "    for each_content in contents:\n",
    "        for i, content_name in enumerate(tmp_list):\n",
    "            try:\n",
    "                p = each_content.select('div p')[0].text\n",
    "            except IndexError:\n",
    "                p = ''\n",
    "\n",
    "            try:\n",
    "                dl = each_content.select('div dl')[0].text\n",
    "            except IndexError:\n",
    "                dl = ''\n",
    "\n",
    "            if content_name == each_content.select('h2')[0].text:\n",
    "                tmp_list[i] = p + dl\n",
    "                if tmp_list[i] == '':\n",
    "                    tmp_list[i] == 'NA'\n",
    "                # if content_name == '工作內容' or content_name == '條件要求':\n",
    "                #     job_skill_data += p.replace('\\n', '').replace('\\t', '')\n",
    "                #     for j in each_content.select('div dl dd[class=\"cate\"] span'):\n",
    "                #         job_skill_data += j.text.replace('\\n', '').replace('\\t', '')\n",
    "\n",
    "    return tmp_list[0], tmp_list[1], tmp_list[2], tmp_list[3], dealWithSynonym(re.sub(r'[-:_0-9、【】：)(，.&+]', '', (tmp_list[0].replace('\\n', '').replace('\\t', '') + tmp_list[1].replace('\\n', '').replace('\\t', ''))))\n",
    "\n",
    "'''\n",
    "Keyword for a list of title and url\n",
    "[[\"Title1\", \"URL1\", \"Skill\"], [\"Title2\", \"URL2\", \"Skill\"]]\n",
    "'''\n",
    "timenow = time.strftime(\"%Y-%m-%d_%H%M\")\n",
    "def keywordForTitle(keyword, max_page = 0, save_separately = 0, cache = 15, from_page = 1):\n",
    "\n",
    "    # Create a directory\n",
    "    path = r'./job104_resource/%s_%s'%(keyword, timenow)\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    work_path = r'./work_dir/%s'%(keyword)\n",
    "    if not os.path.isdir(work_path):\n",
    "        os.mkdir(work_path)\n",
    "\n",
    "    col_path = r'./config/col.txt'\n",
    "    ohencoding_col = open(col_path, 'r').read().lower().split('\\n')\n",
    "\n",
    "    col = ['Job_company', 'Job Openings','Job_content', 'Job_require', 'Job_welfare', 'Job_contact', 'URL']\n",
    "    if len(ohencoding_col) > 0:\n",
    "        col += ohencoding_col\n",
    "    df = pd.DataFrame(columns=col)\n",
    "\n",
    "    pages = from_page\n",
    "    title_url_list = list()\n",
    "    # skill data\n",
    "    job_skill_data_sum = ''\n",
    "    while True:\n",
    "        process_tag = 1\n",
    "\n",
    "        print('Page %s ...\\t==' % (pages), end='')\n",
    "        url = 'https://www.104.com.tw/jobs/search/?ro=0&keyword=%s&order=1&asc=0&page=%s&mode=s&jobsource=2018indexpoc'%(keyword, pages)\n",
    "        try:\n",
    "            res = requests.get(url, headers=headers)\n",
    "        except:\n",
    "            print('Error', 'keywordForTitle(keyword, max_page = 0)', url)\n",
    "            print('Wait a moment!')\n",
    "            time.sleep(4)\n",
    "            continue\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        title = soup.select('div[class=\"b-block__left\"] h2[class=\"b-tit\"] a')\n",
    "        company = soup.select('div[class=\"b-block__left\"] ul[class=\"b-list-inline b-clearfix\"] li a')\n",
    "        print('==', end='')\n",
    "\n",
    "        # Stoping conditions\n",
    "        # If the page is empty -> stop\n",
    "        if len(title) == 0:\n",
    "            if process_tag < 30:\n",
    "                print('  ' * (30 - process_tag), end='')\n",
    "            print('Empty!')\n",
    "            break\n",
    "\n",
    "        # [[\"Title1\", \"URL1\"], [\"Title2\", \"URL2\"]]\n",
    "        for num, each_title in enumerate(title):\n",
    "            ohencoding_col_tmp = []\n",
    "            if len(ohencoding_col) > 0:\n",
    "                ohencoding_col_tmp = [0 for item in ohencoding_col]\n",
    "            tmp_title = each_title.text\n",
    "            tmp_url = each_title['href'].replace('//', 'https://')\n",
    "            # job_skill_data is a list of all skills\n",
    "            job_content, job_require, job_welfare, job_contact, job_skill_data = getSkill(tmp_url)\n",
    "            # skill data\n",
    "            job_skill_data_sum += (job_skill_data + ',')\n",
    "\n",
    "            # Do one-hot encoding\n",
    "            title_url_list.append([tmp_title, tmp_url])\n",
    "            for index, skill_col in enumerate(ohencoding_col):\n",
    "                for check_skill in job_skill_data.split(','):\n",
    "                    if check_skill.upper() == skill_col.upper():\n",
    "                        ohencoding_col_tmp[index] = 1\n",
    "                        continue\n",
    "            tmp_col = [company[num].text, tmp_title, job_content, job_require, job_welfare, job_contact, tmp_url] + ohencoding_col_tmp\n",
    "            df = df.append(pd.DataFrame([tmp_col], columns=col), ignore_index=True)\n",
    "            print('==', end='')\n",
    "            process_tag += 1\n",
    "            time.sleep(random.randint(3,8)/10)\n",
    "\n",
    "         # save skill data to a file every 15 pages\n",
    "        if pages % cache == 0:\n",
    "            with open(r'%s/%s'%(work_path, pages), 'w', encoding='utf-8') as skill:\n",
    "                skill.write(job_skill_data_sum)\n",
    "            job_skill_data_sum = ''\n",
    "\n",
    "            if save_separately != 0:\n",
    "                df.to_excel(r'%s/title_url_%s.xlsx' % (path, time.strftime(\"%Y-%m-%d_%H%M\")), engine='xlsxwriter')\n",
    "                col = ['Job_company', 'Job Openings', 'Job_content', 'Job_require', 'Job_welfare', 'Job_contact', 'URL']\n",
    "                if len(ohencoding_col) > 0:\n",
    "                    col += ohencoding_col\n",
    "                df = pd.DataFrame(columns=col)\n",
    "\n",
    "        # Stoping conditions\n",
    "        if max_page != 0:\n",
    "            if pages >= max_page:\n",
    "                if process_tag < 30:\n",
    "                    print('==' * (30 - process_tag), end='')\n",
    "                print('Done!')\n",
    "                break\n",
    "\n",
    "        if process_tag < 30:\n",
    "            print('=='*(30-process_tag), end='')\n",
    "        print('Done!')\n",
    "        if pages % cache == 0 and save_separately != 0:\n",
    "            print('File has saved to %s' % (path))\n",
    "\n",
    "        # Pause for 30 sec every 15 pages\n",
    "        if pages % 15 == 0:\n",
    "            print('----------\\nTake a break for 30 sec.\\n----------')\n",
    "            time.sleep(30)\n",
    "        pages += 1\n",
    "\n",
    "    with open(r'%s/%s' % (work_path, pages), 'w', encoding='utf-8') as skill:\n",
    "        skill.write(job_skill_data_sum)\n",
    "\n",
    "    df.to_excel(r'%s/title_url_%s.xlsx'%(path, time.strftime(\"%Y-%m-%d_%H%M\")), engine='xlsxwriter')\n",
    "    print('File has saved to %s/title_url.xlsx'%(path))\n",
    "    return title_url_list\n",
    "\n",
    "# Count title amount\n",
    "def keywordForTitle_countTitle(keyword, max_page = 0, from_page = 1):\n",
    "\n",
    "    pages = from_page\n",
    "    count_title = 0\n",
    "    while True:\n",
    "        url = 'https://www.104.com.tw/jobs/search/?ro=0&keyword=%s&order=1&asc=0&page=%s&mode=s&jobsource=2018indexpoc'%(keyword, pages)\n",
    "        res = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        title = soup.select('div[class=\"b-block__left\"] h2[class=\"b-tit\"] a')\n",
    "\n",
    "        # Stoping conditions\n",
    "        # If the page is empt -> stop\n",
    "        if len(title) == 0:\n",
    "            break\n",
    "\n",
    "        # [[\"Title1\", \"URL1\"], [\"Title2\", \"URL2\"]]\n",
    "        for each_title in title:\n",
    "            count_title += 1\n",
    "\n",
    "        # Stoping conditions\n",
    "        if max_page != 0:\n",
    "            if pages >= max_page:\n",
    "                break\n",
    "        pages += 1\n",
    "    return count_title\n",
    "\n",
    "# Map reduce\n",
    "def mrThread(file_path, mr_path, save_name):\n",
    "    mr_dict = {}\n",
    "    tmp_str = ''\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        tmp_list = f.read().replace('\\n','').split(',')\n",
    "    for w in tmp_list:\n",
    "        if w in mr_dict:\n",
    "            mr_dict[w] += 1\n",
    "        else:\n",
    "            mr_dict[w] = 1\n",
    "\n",
    "    for d in mr_dict:\n",
    "        tmp_str += '%s:%s\\n'%(d, mr_dict[d])\n",
    "\n",
    "    with open(r'%s/%s'%(mr_path, save_name), 'w', encoding='utf-8') as f:\n",
    "        f.write(tmp_str)\n",
    "    # return tmp_str\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kyword = '大數據分析'\n",
    "    pages = 0\n",
    "    save_separately = 1\n",
    "    cache = 15\n",
    "    ori_par = [kyword, pages, save_separately, cache]\n",
    "    with open(r'./config/conf.txt', 'r') as con:\n",
    "        par = con.read().split('\\n')\n",
    "    # print(par)\n",
    "    for n, p in enumerate(par):\n",
    "        par[n] = p.split('=')[1].replace(' ', '')\n",
    "        if n != 0:\n",
    "            par[n] = int(par[n])\n",
    "        if n == 1 or n == 2:\n",
    "            if par[n] < 0:\n",
    "                par[n] = ori_par[n]\n",
    "        if n == 3:\n",
    "            if par[n] < 1:\n",
    "                par[n] = ori_par[n]\n",
    "    # print(par)\n",
    "    kyword = par[0]\n",
    "    pages = par[1]\n",
    "    save_separately = par[2]\n",
    "    cache = par[3]\n",
    "\n",
    "    print('[Config]')\n",
    "    print('\\tKeyword:\\t\\t\\t%s'%(kyword))\n",
    "    if pages == 0:\n",
    "        print('\\tPages:\\t\\t\\t\\t%s' % ('ALL'))\n",
    "    else:\n",
    "        print('\\tPages:\\t\\t\\t\\t%s' % (pages))\n",
    "    print('\\tSave separately:\\t\\t%s' % (save_separately))\n",
    "    print('\\tCache:\\t\\t\\t\\t%s' % (cache))\n",
    "    print('\\n')\n",
    "\n",
    "    print('[Querying for %s ...]'%(kyword))\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Compute the amount of title\n",
    "    print('Computing the amount of title... ', end='')\n",
    "    title_amount = keywordForTitle_countTitle(kyword, pages)\n",
    "    print('(%s)'%(title_amount))\n",
    "    print('\\n')\n",
    "    time.sleep(2)\n",
    "\n",
    "    print('[Loading data...]')\n",
    "    # Crawl all title, url and content and save as file\n",
    "    keyword_for_title_and_url = keywordForTitle(kyword, pages, save_separately, cache)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Print data\n",
    "    # pprint.pprint(keyword_for_title_and_url)\n",
    "    print('')\n",
    "    print('[Done!]')\n",
    "\n",
    "    # Map-reduce\n",
    "    time.sleep(1)\n",
    "    print('\\n')\n",
    "    print('[Computing the amount of each skill...]')\n",
    "\n",
    "    work_path = r'./work_dir/%s' % (kyword)\n",
    "    mr_path = r'%s/mr_%s' % (work_path, time.strftime(\"%Y-%m-%d_%H%M\"))\n",
    "\n",
    "    # cache_list = os.listdir(work_path)\n",
    "    cache_list = [f for f in os.listdir(work_path) if f[0:2] != 'mr']\n",
    "    threadList = list()\n",
    "    for save_name, thr in enumerate(cache_list):\n",
    "        threadList.append(threading.Thread(target=mrThread, args=(r'%s/%s' % (work_path, thr), mr_path, save_name)))\n",
    "\n",
    "    if not os.path.isdir(mr_path):\n",
    "        os.mkdir(mr_path)\n",
    "\n",
    "    for i in threadList:\n",
    "        i.start()\n",
    "        # time.sleep(0.1)\n",
    "    for i in threadList:\n",
    "        i.join()\n",
    "\n",
    "    tmp_mr = []\n",
    "    tmp_mr_dict = {}\n",
    "    for i in os.listdir(mr_path):\n",
    "        with open(r'%s/%s'%(mr_path, i), 'r', encoding='utf-8') as f:\n",
    "            tmp_mr += f.read().split('\\n')\n",
    "    # print(tmp_mr)\n",
    "    for i in tmp_mr:\n",
    "        if len(i.split(':')) != 2 or i.split(':')[0] == '':\n",
    "            continue\n",
    "        if i.split(':')[0] in tmp_mr_dict:\n",
    "            tmp_mr_dict[i.split(':')[0]] += int(i.split(':')[1])\n",
    "        else:\n",
    "            tmp_mr_dict[i.split(':')[0]] = int(i.split(':')[1])\n",
    "\n",
    "    tmp_str = ''\n",
    "    for d in tmp_mr_dict:\n",
    "        tmp_str += '%s:%s\\n'%(d, tmp_mr_dict[d])\n",
    "\n",
    "    path = r'./job104_resource/%s_%s' % (kyword, timenow)\n",
    "    with open(r'%s/map_reduce_%s.txt'%(path, timenow), 'w', encoding='utf-8') as f:\n",
    "        f.write(tmp_str)\n",
    "\n",
    "    # Remove temporary MR file\n",
    "    for rm in cache_list:\n",
    "        os.remove(r'%s/%s'%(work_path, rm))\n",
    "\n",
    "    print('Processes all done.\\n')\n",
    "    print('Check the following directories.')\n",
    "    print('./job104_resource/%s_%s'%(kyword, timenow))\n",
    "\n",
    "    time.sleep(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Tensorflow\\\\Pyetl-104'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[],[],[],[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
